---
title: "Variational Bayesian Inference"
author: ''
date: '2021-04-27'
slug: variational-bayesian-inference
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-04-27T13:22:47-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
draft: true
---



<p>Potential resources</p>
<ul>
<li><a href="https://fabiandablander.com/r/Variational-Inference.html#fn:1">A Brief Primer on Variational Inference</a></li>
<li><a href="https://rpubs.com/cakapourani/variational-bayes-lr">Variational Bayesian Linear Regerssion</a></li>
<li><a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational Bayesian Inference</a></li>
<li><a href="https://en.wikipedia.org/wiki/Expectation_propagation">Expectation Propogation</a></li>
<li><a href="https://rpubs.com/cakapourani/variational_bayes_gmm">Variational Mixture of Gaussians</a></li>
<li><a href="https://towardsdatascience.com/variational-gaussian-process-what-to-do-when-things-are-not-gaussian-41197039f3d4">Variational Gaussian processes</a></li>
<li><a href=""></a></li>
</ul>
<p>The goal of variational inference is to replace the computationally expensive task of full Bayesian inference using Markov Chain Monte Carlo (MCMC) with a less computationally expensive optimization of an approximate distribution.</p>
<p>Given a likelihood <span class="math inline">\(p(\mathbf{y} | \boldsymbol{\theta})\)</span> and prior <span class="math inline">\(p(\boldsymbol{\theta})\)</span>, the posterior distribution is</p>
<p><span class="math display">\[\begin{align*}
p(\boldsymbol{\theta} | \mathbf{y}) = \frac{p(\mathbf{y} | \boldsymbol{\theta}) p(\boldsymbol{\theta})}{\int p(\mathbf{y} | \boldsymbol{\theta}) p(\boldsymbol{\theta}) d \boldsymbol{\theta}}
\end{align*}\]</span></p>
<p>In general, calculating (or sampling from) the posterior distribution can be computationally challenging.</p>
<div id="working" class="section level1">
<h1>Working</h1>
<p>It can be computationally challenging to sample from the posterior <span class="math inline">\(p(\mathbf{y} | \boldsymbol{\theta})\)</span> directly. Instead, we can find a distribution <span class="math inline">\(q^\star(\boldsymbol{\theta})\)</span> that is chosen from a family of densities <span class="math inline">\(\mathcal{Q}\)</span> where <span class="math inline">\(q^\star(\boldsymbol{\theta})\)</span> is the distribution in <span class="math inline">\(\mathcal{Q}\)</span> that best approximates the posterior distribution <span class="math inline">\(p(\mathbf{y} | \boldsymbol{\theta})\)</span> according to the Kullback-Leibler divergence
<span class="math display">\[
\begin{align*}
q^\star(\boldsymbol{\theta}) &amp; = \underset{q(\boldsymbol{\theta}) \in \mathcal{Q}}{arg\,min}\, KL(q(\boldsymbol{\theta}) | p(\mathbf{y} | \boldsymbol{\theta})),
\end{align*}
\]</span>
where the Kullback-Leibler divergence is
<span class="math display">\[
\begin{align*}
KL(q(\boldsymbol{\theta}) | p(\mathbf{y} | \boldsymbol{\theta})) &amp; =  \int q(\boldsymbol{\theta}) \log \left( \frac{q(\boldsymbol{\theta})}{p(\mathbf{y} | \boldsymbol{\theta})} \right) d \boldsymbol{\theta} \\
&amp; = \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( \frac{q(\boldsymbol{\theta})}{p(\mathbf{y} | \boldsymbol{\theta})} \right).
\end{align*}
\]</span>
The Kullback-Leibler (KL) divergence measures the asymmetric difference between the densities <span class="math inline">\(q(\boldsymbol{\theta})\)</span> and <span class="math inline">\(p(\boldsymbol{\theta} | \mathbf{y})\)</span> (Note: <span class="math inline">\(KL(q(\boldsymbol{\theta}) | p(\mathbf{y} | \boldsymbol{\theta})) \neq KL(p(\mathbf{y} | \boldsymbol{\theta})| q(\boldsymbol{\theta}))\)</span> and therefore the KL divergence is not a metric).</p>
<p>Notice that this representation does not solve the computational challenge as the marginal density of the data <span class="math inline">\(p(\mathbf{y}) = \int p(\mathbf{y} | \boldsymbol{\theta}) p(\boldsymbol{\theta}) d \boldsymbol{\theta}\)</span> requires a high-dimensional integral.</p>
<p>Using the definition of conditional probability <span class="math inline">\(p(\boldsymbol{\theta} | \mathbf{y}) = \frac{p(\mathbf{y}, \boldsymbol{\theta})} {p(\mathbf{y})}\)</span>, the Kullback-Leibler divergence can be written as
<span class="math display">\[
\begin{align*}
 KL(q(\boldsymbol{\theta}) | p(\mathbf{y} | \boldsymbol{\theta})) &amp; = \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( \frac{q(\boldsymbol{\theta})}{p(\mathbf{y} | \boldsymbol{\theta}} \right) \\
&amp; = \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( q(\boldsymbol{\theta} ) \right) - \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( p(\mathbf{y} | \boldsymbol{\theta} ) \right) \\
&amp; = \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( q(\boldsymbol{\theta}) \right) - \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( \frac{p(\mathbf{y}, \boldsymbol{\theta})} {p(\mathbf{y})} \right) \\
&amp; = \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( q(\boldsymbol{\theta}) \right) - \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) + \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( p(\mathbf{y}) \right) \\
&amp; = \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( q(\boldsymbol{\theta}) \right) - \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) + \log \left( p(\mathbf{y}) \right),
\end{align*}
\]</span>
where the last term in the last equation is the log of the marginal density of the data which is given by the high dimensional integral <span class="math inline">\(p(\mathbf{y}) = \int p(\mathbf{y} | \boldsymbol{\theta}) p(\boldsymbol{\theta}) d \boldsymbol{\theta}\)</span> and is a constant with respect to the approximating distribution <span class="math inline">\(q(\boldsymbol{\theta})\)</span> and is ignorable in the optimization (Note: this distinction leads to the variational approximation underestimating the posterior variance).</p>
<p>Define <span class="math inline">\(ELBO(q)\)</span> as the evidence lower bound with respect to the approximating distribution as
<span class="math display">\[
\begin{align*}
ELBO(q) &amp; = - \left(  KL(q(\boldsymbol{\theta}) | p(\mathbf{y} | \boldsymbol{\theta})) - \log \left( p(\mathbf{y}) \right) \right).
\end{align*}
\]</span>
We can re-write this equation to be a function of the log <a href="https://en.wikipedia.org/wiki/Marginal_likelihood">marignal likelihood</a> of the data <span class="math inline">\(\log \left( p(\mathbf{y} ) \right)\)</span> which is also called the evidence. Thus, the evidence can be written as</p>
<p><span class="math display">\[
\begin{align*}
\log \left( p(\mathbf{y}) \right) &amp; = ELBO(q) + KL(q(\boldsymbol{\theta}) | p(\mathbf{y} | \boldsymbol{\theta})) \\
&amp; \geq ELBO(q),
\end{align*}
\]</span>
because the Kullback-Leibler divergence <span class="math inline">\(KL(q(\boldsymbol{\theta}) | p(\mathbf{y} | \boldsymbol{\theta}))\)</span> is non-negative. Thus, the evidence <span class="math inline">\(\log \left( p(\mathbf{y} )\right)\)</span> is bounded below by the <span class="math inline">\(ELBO(q)\)</span>. Thus, making the <span class="math inline">\(ELBO(q)\)</span> as large as possible results in minimizing the Kullback-Leibler divergence <span class="math inline">\(KL(q(\boldsymbol{\theta}) | p(\mathbf{y} | \boldsymbol{\theta}))\)</span>.</p>
</div>
<div id="understanding-the-elboq-as-a-penalized-optimization" class="section level1">
<h1>Understanding the <span class="math inline">\(ELBO(q)\)</span> as a penalized optimization</h1>
<p><span class="math display">\[
\begin{align*}
ELBO(q) &amp; = - \left(  KL(q(\boldsymbol{\theta}) | p(\mathbf{y} | \boldsymbol{\theta})) - \log \left( p(\mathbf{y} \right) \right) \\
&amp; = - \left( \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( q(\boldsymbol{\theta}) \right) - \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) + \log \left( p(\mathbf{y}) \right)  - \log \left( p(\mathbf{y} ) \right) \right) \\
&amp; = \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) - \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( q(\boldsymbol{\theta}) \right) 
\end{align*}
\]</span></p>
<p>Using the definition of conditional probability <span class="math inline">\(p(\boldsymbol{\theta} | \mathbf{y}) = \frac{p(\mathbf{y}, \boldsymbol{\theta})} {p(\mathbf{y})}\)</span> again, we can write <span class="math inline">\(ELBO(q)\)</span> as
<span class="math display">\[
\begin{align*}
ELBO(q) &amp; = \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) - \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( q(\boldsymbol{\theta}) \right) \\
&amp; = \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( p(\mathbf{y} | \boldsymbol{\theta}) \right) + \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( p(\boldsymbol{\theta}) \right) - \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( q(\boldsymbol{\theta}) \right) \\
&amp; = \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( p(\mathbf{y} | \boldsymbol{\theta}) \right) + \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( \frac{p(\boldsymbol{\theta})}{q(\boldsymbol{\theta})} \right) \\
&amp; = \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( p(\mathbf{y} | \boldsymbol{\theta}) \right) - \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( \frac{q(\boldsymbol{\theta})}{p(\boldsymbol{\theta})} \right) \\
&amp; = \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( p(\mathbf{y} | \boldsymbol{\theta}) \right) - KL(q(\boldsymbol{\theta}) | p(\boldsymbol{\theta})),
\end{align*}
\]</span>
where the maximization of the <span class="math inline">\(ELBO(q)\)</span> is now expressed as an optimization over the distributions <span class="math inline">\(q(\boldsymbol{\theta})\)</span> that maximize the log-likelihood of the data <span class="math inline">\(p(\mathbf{y} | \boldsymbol{\theta})\)</span> subject to a penalty term if the approximating distribution <span class="math inline">\(q(\boldsymbol{\theta})\)</span> is far from the prior distribution <span class="math inline">\(p(\boldsymbol{\theta})\)</span> with respect to the Kullback-Leibler divergence.</p>
</div>
<div id="finding-the-optimal-qstarboldsymboltheta" class="section level1">
<h1>Finding the optimal <span class="math inline">\(q^\star(\boldsymbol{\theta})\)</span></h1>
<p>In a usual optimization problem, the goal is to find the value of <span class="math inline">\(\boldsymbol{\theta}\)</span> that maximizes the log-likelihood. In contrast, the variational optimization finds the functional <span class="math inline">\(q(\boldsymbol{\theta})\)</span> that minimizes the Kullback-Leibler divergence. A functional is a function that takes as inputs a function and returns either a single value or a function as an output.</p>
<p>To perform the optimization, we want to find the function <span class="math inline">\(q(\boldsymbol{\theta}) \in \mathcal{Q}\)</span> that minimizes the functional <span class="math inline">\(ELBO(q)\)</span>. In general, to make the optimization computationally tractable, we find a solution to a constrained family of functions. A possible approach is to assume is that <span class="math inline">\(\mathcal{Q}\)</span> is a family of Gaussian distributions where <span class="math inline">\(q(\boldsymbol{\theta})\)</span> with parameter vector <span class="math inline">\(\boldsymbol{\gamma}\)</span>. In this setting, the <span class="math inline">\(ELBO(q)\)</span> is thus a function of <span class="math inline">\(\boldsymbol{\gamma}\)</span> and maximizing the <span class="math inline">\(ELBO(q)\)</span> becomes a usual optimization.</p>
<div id="mean-field-variational-inference" class="section level2">
<h2>Mean field variational inference</h2>
<p>The mean field approximation is named because we model the <span class="math inline">\(d\)</span> dimensional latent vector <span class="math inline">\(\boldsymbol{\theta} = (\theta_1, \ldots, \theta_d)&#39;\)</span> through its mean (and not its covariance) by assuming that for <span class="math inline">\(j = 1, \ldots, d\)</span> the approximating distributions <span class="math inline">\(q(\boldsymbol{\theta})\)</span> can be factored into independent distributions
<span class="math display">\[
\begin{align*}
q(\boldsymbol{\theta}) = \prod_{j=1}^d q_j(\theta_j)
\end{align*}
\]</span>
where the functional form for <span class="math inline">\(q_j(\theta_j)\)</span> can be derived for each parameter <span class="math inline">\(\theta_j\)</span>.</p>
<p>Under the mean field assumption, the <span class="math inline">\(ELBO(q)\)</span> can be written as
<span class="math display">\[
\begin{align*}
ELBO(q) &amp; = \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) - \operatorname{E}_{q(\boldsymbol{\theta})} \log \left( q(\boldsymbol{\theta}) \right) \\
&amp; = \int q(\boldsymbol{\theta}) \log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) d \boldsymbol{\theta} - \int q(\boldsymbol{\theta}) \log \left( q(\boldsymbol{\theta}) \right) d \boldsymbol{\theta} \\
&amp; = \int \prod_{j=1}^d q_j(\theta_j) \log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) d \boldsymbol{\theta} - \int \prod_{j=1}^d q_j(\theta_j) \log \left( \prod_{j=1}^d q_j(\theta_j) \right) d \boldsymbol{\theta} 
\end{align*}
\]</span>
Optimizing the <span class="math inline">\(ELBO(q)\)</span> can be done using coordinate ascent variational inference where the optimization is performed using the conditional posterior distribution for each component <span class="math inline">\(\theta_j\)</span> given the other components <span class="math inline">\(\boldsymbol{\theta}_{-j}\)</span> where <span class="math inline">\(\boldsymbol{\theta}_{-j} = (\theta_1, \ldots, \theta_{j-1}, \theta_{j+1}, \ldots, \theta_d)&#39;\)</span> which is the <span class="math inline">\(d-1\)</span> dimensional vector of all the elements of <span class="math inline">\(\boldsymbol{\theta}\)</span> except for the <span class="math inline">\(j\)</span>th element. Thus the <span class="math inline">\(ELBO(q_j)\)</span> with respect to the <span class="math inline">\(j\)</span>th element <span class="math inline">\(\theta_j\)</span> of <span class="math inline">\(\boldsymbol{\theta}\)</span> while holding all other <span class="math inline">\(\boldsymbol{\theta}_{-j}\)</span> constant is
<span class="math display">\[
\begin{align*}
ELBO(q_j) &amp; = \int \prod_{j=1}^d q_j(\theta_j) \log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) d \boldsymbol{\theta} - \int \prod_{j=1}^d q_j(\theta_j) \log \left( \prod_{j=1}^d q_j(\theta_j) \right) d \boldsymbol{\theta} \\
 &amp; = \int \prod_{j=1}^d q_j(\theta_j) \log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) d \boldsymbol{\theta} - \int q_j(\theta_j) \log \left( q_j(\theta_j) \right) d \theta_j  - \int \prod_{k\neq j} q_k(\theta_k) \log \left( \prod_{k \neq j} q_k(\theta_k) \right) d \boldsymbol{\theta}_{-j} &amp; \color{red}{\mbox{Don&#39;t understand this step yet--not convinced this is correct}}\\
\end{align*}
\]</span>
Noticing that <span class="math inline">\(\int \prod_{k\neq j} q_k(\theta_k) \log \left( \prod_{k \neq j} q_k(\theta_k) \right) d \boldsymbol{\theta}_{-j}\)</span> is constant with respect to <span class="math inline">\(q_j(\theta_j)\)</span>, this term can be dropped from the optimization giving
<span class="math display">\[
\begin{align*}
ELBO(q_j) &amp; \propto \int \prod_{j=1}^d q_j(\theta_j) \log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) d \boldsymbol{\theta} - \int q_j(\theta_j) \log \left( q_j(\theta_j) \right) d \theta_j  \\
&amp; = \int q_j(\theta_j) \left( \int \prod_{k \neq j} q_k(\theta_k) \log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) d \boldsymbol{\theta}_j \right) d \theta_j - \int q_j(\theta_j) \log \left( q_j(\theta_j) \right) d \theta_j  \\
&amp; = \int q_j(\theta_j) E_{q(\boldsymbol{\theta}_{-j})} \left(\log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) \right) d \theta_j - \int q_j(\theta_j) \log \left( q_j(\theta_j) \right) d \theta_j.
\end{align*}
\]</span>
Maximizing the <span class="math inline">\(ELBO(q_j)\)</span> requires evaluating the expectation <span class="math inline">\(E_{q(\boldsymbol{\theta}_{-j})} \left(\log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) \right)\)</span>. Define the distribution
<span class="math display">\[
\begin{align*}
\log \left( \tilde{p}(\mathbf{y}, \theta_j) \right) &amp; = E_{q(\boldsymbol{\theta}_{-j})} \left(\log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) \right) - \mathcal{Z}
\end{align*}
\]</span>
where <span class="math inline">\(\mathcal{Z}\)</span> is a normalizing constant that ensures the distribution <span class="math inline">\(\tilde{p}(\mathbf{y}, \theta_j)\)</span> integrates to one. Then, we have
<span class="math display">\[
\begin{align*}
ELBO(q_j) &amp; \propto \int q_j(\theta_j) E_{q(\boldsymbol{\theta}_{-j})} \left(\log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) \right) d \theta_j - \int q_j(\theta_j) \log \left( q_j(\theta_j) \right) d \theta_j \\
&amp; = \int q_j(\theta_j) \log \left( \tilde{p}(\mathbf{y}, \theta_j) \right)  d \theta_j - \int q_j(\theta_j) \log \left( q_j(\theta_j) \right) d \theta_j \\
&amp; = \int q_j(\theta_j) \log \left( \frac{\tilde{p}(\mathbf{y}, \theta_j)}{q_j(\theta_j)} \right)  d \theta_j \\
&amp; = - \int q_j(\theta_j) \log \left( \frac{q_j(\theta_j)}{\tilde{p}(\mathbf{y}, \theta_j)} \right)  d \theta_j \\
&amp; = - KL \left( q_j(\theta_j) | \tilde{p}(\mathbf{y}, \theta_j) \right),
\end{align*}
\]</span>
which shows that maximizing the ELBO with respect to <span class="math inline">\(q_j(\theta_j)\)</span> is equivalent to minimizing the Kullback-Leibler divergence between <span class="math inline">\(q_j(\theta_j)\)</span> and <span class="math inline">\(\tilde{p}(\mathbf{y}, \theta_j)\)</span>. When <span class="math inline">\(q_j(\theta_j) = \tilde{p}(\mathbf{y}, \theta_j)\)</span>, the Kullback-Leibler divergence is at its minimum value of zero. Therefore, the distribution that maximizes the density <span class="math inline">\(q_j(\theta_j)\)</span> conditional on the distributions <span class="math inline">\(\{q_1(\theta_1), \ldots, q_{j-1}(\theta_{j-1}), q_{j+1}(\theta_{j+1}), \ldots, q_d(\theta_d)\}\)</span> is
<span class="math display">\[
\begin{align*}
q^\star_j(\theta_j) &amp; = \exp\left( E_{q(\boldsymbol{\theta}_{-j})} \left(\log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) \right) - \mathcal{Z} \right) \\
&amp; = \frac{\exp\left( E_{q(\boldsymbol{\theta}_{-j})} \left(\log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) \right) \right)}{\int \exp\left( E_{q(\boldsymbol{\theta}_{-j})} \left(\log \left( p(\mathbf{y}, \boldsymbol{\theta}) \right) \right) d \theta_j \right)}
\end{align*}
\]</span></p>
</div>
</div>
<div id="example-simple-linear-regression" class="section level1">
<h1>Example: Simple Linear regression</h1>
<p>For <span class="math inline">\(i = 1, \ldots, n\)</span>, let <span class="math inline">\(y_i\)</span> be an observation with mean <span class="math inline">\(\mu\)</span> and associated covariate <span class="math inline">\(x_i\)</span> with slope <span class="math inline">\(\beta\)</span>. Then, the linear regression model can be written as
<span class="math display">\[
\begin{align*}
y_i &amp; \sim N(\mu + x_i \beta, \sigma^2)
\end{align*}
\]</span>
where the parameters <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> are assigned priors
<span class="math display">\[
\begin{align*}
\mu &amp; \sim N(\mu_\mu, \sigma^2 \sigma^2_\mu) \\
\beta &amp; \sim N(\mu_\beta, \sigma^2 \sigma^2_\beta) \\
\sigma^2 &amp; \sim \operatorname{inverse-gamma}(\alpha_{\sigma^2}, \beta_{\sigma^2})
\end{align*}
\]</span></p>
<div id="determining-the-variational-densities" class="section level3">
<h3>Determining the variational densities</h3>
</div>
<div id="variational-density-for-beta" class="section level2">
<h2>Variational density for <span class="math inline">\(\beta\)</span></h2>
<p>The optimal variational density <span class="math inline">\(q^\star(\beta)\)</span> is given by
<span class="math display">\[
\begin{align*}
q^\star(\beta) \propto \exp \left( E_{q(\mu), q(\sigma^2)} \left( \log \left( p(\beta | \mathbf{y}, \mu, \sigma^2 ) \right) \right) \right) 
\end{align*}
\]</span>
which requires finding the full conditional distribution <span class="math inline">\(p(\beta | \mathbf{y}, \mu, \sigma^2 )\)</span>. The full conditional distribution is
<span class="math display">\[
\begin{align*}
p(\beta | \mathbf{y}, \mu, \sigma^2 ) &amp; \propto N(a^{-1}b, a^{-1}) \mbox{ where } \\
a &amp; = \frac{1}{\sigma^2} \left( \sum_{i=1}^n x_i^2 + \frac{1}{\sigma^2_\beta} \right) \\
b &amp; = \frac{1}{\sigma^2} \left( \sum_{i=1}^n x_i (y_i - \mu) + \frac{\mu_\beta}{\sigma^2_\beta} \right)
\end{align*}
\]</span>
Thus, the variational density is
<span class="math display">\[
\begin{align*}
q^\star(\beta) &amp; \propto \exp \left( E_{q(\sigma^2), q(\mu)} \left( \log \left( p(\beta | \mathbf{y}, \mu, \sigma^2 ) \right) \right) \right) \\
&amp; \propto \exp \left(  E_{q(\sigma^2), q(\mu)} \left( -\frac{1}{2} a \left( \beta - a^{-1} b \right)^2 \right) \right)  \\
&amp; = \exp \left(  - \frac{1}{2} E_{q(\sigma^2)} \left( \frac{1}{\sigma^2} \right) \left( \sum_{i=1}^n x_i^2 + \frac{1}{\sigma^2_\beta} \right)  E_{q(\mu)} \left( \left( \beta - \frac{\sum_{i=1}^n x_i \left( y_i - \mu \right) + \frac{\mu_\beta}{\sigma^2_\beta}}{ \sum_{i=1}^n x_i^2 + \frac{1}{\sigma^2_\beta}} \right)^2 \right) \right) \\
\end{align*}
\]</span>
which is proportional to a Gaussian distribution with mean
<span class="math display">\[
\begin{align*}
&amp; E_{q(\sigma^2), q(\mu)} \left( \sigma^2 \left( \sum_{i=1}^n x_i^2 + \frac{1}{\sigma^2_\beta} \right)^{-1} \frac{1}{\sigma^2} \left( \sum_{i=1}^n x_i \left( y_i - \mu \right) + \frac{\mu_\beta}{\sigma^2_\beta} \right) \right)  \\
&amp; =  \left( \sum_{i=1}^n x_i^2 + \frac{1}{\sigma^2_\beta} \right)^{-1} \left( \sum_{i=1}^n x_i \left( y_i -  E_{q(\mu)} \left( \mu \right) \right) + \frac{\mu_\beta}{\sigma^2_\beta} \right)
\end{align*}
\]</span>
and variance <span class="math inline">\(E_{q(\sigma^2)} \left( \frac{1}{\sigma^2} \right) \left( \sum_{i=1}^n x_i^2 + \frac{1}{\sigma^2_\beta} \right)^{-1}\)</span> where the mean term depends on <span class="math inline">\(E_{q(\mu)} (\mu)\)</span> and the variance term depends on <span class="math inline">\(E_{q(\sigma^2)} (\frac{1}{\sigma^2})\)</span>. Thus, to evaluate this density, we have to also find the variational densities <span class="math inline">\(q(\sigma^2)\)</span> and <span class="math inline">\(q(\mu)\)</span>.</p>
</div>
<div id="variational-density-for-mu" class="section level2">
<h2>Variational density for <span class="math inline">\(\mu\)</span></h2>
<p>The optimal variational density <span class="math inline">\(q^\star(\mu)\)</span> is given by
<span class="math display">\[
\begin{align*}
q^\star(\mu) \propto \exp \left( E_{q(\sigma^2), q(\beta)} \left( \log \left( p(\mu | \mathbf{y}, \beta, \sigma^2 ) \right) \right) \right)
\end{align*}
\]</span>
which requires finding the full conditional distribution <span class="math inline">\(p(\mu | \mathbf{y}, \beta, \sigma^2 )\)</span>. The full conditional distribution is
<span class="math display">\[
\begin{align*}
p(\mu | \mathbf{y}, \mu, \sigma^2 ) &amp; \propto N(a^{-1}b, a^{-1}) \mbox{ where } \\
a &amp; = \frac{1}{\sigma^2} \left(n + \frac{1}{\sigma^2_\mu} \right) \\
b &amp; = \frac{1}{\sigma^2} \left( \sum_{i=1}^n (y_i - x_i \beta) + \frac{\mu_\mu}{\sigma^2_\mu} \right)
\end{align*}
\]</span>
Thus, the variational density is
<span class="math display">\[
\begin{align*}
q^\star(\mu) &amp; \propto \exp \left( E_{q(\sigma^2), q(\beta)} \left( \log \left( p(\mu | \mathbf{y}, \beta, \sigma^2 ) \right) \right) \right) \\
&amp; = \exp \left(  E_{q(\sigma^2), q(\beta)} \left( -\frac{1}{2} a \left( \mu - a^{-1} b \right)^2 \right) \right)  \\
&amp; = \exp \left(  - \frac{1}{2} E_{q(\sigma^2)} \left( \frac{1}{\sigma^2} \right) \left( n + \frac{1}{\sigma^2_\mu} \right) E_{q(\beta)} \left( \left( \mu - \frac{\sum_{i=1}^n (y_i - x_i \beta) + \frac{\mu_\mu}{\sigma^2_\mu}}{ n + \frac{1}{\sigma^2_\mu}} \right)^2 \right) \right) \\
\end{align*}
\]</span>
which is proportional to a Gaussian distribution with mean
<span class="math display">\[
\begin{align*}
&amp; E_{q(\sigma^2), q(\beta)} \left( \sigma^2 \left( n + \frac{1}{\sigma^2_\mu} \right)^{-1} \frac{1}{\sigma^2} \left( \sum_{i=1}^n \left( y_i - x_i \beta \right) + \frac{\mu_\mu}{\sigma^2_\mu} \right) \right)  \\
&amp; =\left( n + \frac{1}{\sigma^2_\mu} \right)^{-1} \left( \sum_{i=1}^n \left( y_i - x_i E_{q(\beta)} \left( \beta \right) \right) + \frac{\mu_\mu}{\sigma^2_\mu} \right)
\end{align*}
\]</span>
and variance <span class="math inline">\(E_{q(\sigma^2)} (\frac{1}{\sigma^2}) \left( n + \frac{1}{\sigma^2_\mu} \right)^{-1}\)</span> where the mean term depends on <span class="math inline">\(E_{q(\beta)} (\beta)\)</span> and the variance term depends on <span class="math inline">\(E_{q(\sigma^2)} (\frac{1}{\sigma^2})\)</span>. Thus, to evaluate this density, we have to also find the variational densities <span class="math inline">\(q(\sigma^2)\)</span> and <span class="math inline">\(q(\beta)\)</span>.</p>
</div>
<div id="variational-density-for-sigma2" class="section level2">
<h2>Variational density for <span class="math inline">\(\sigma^2\)</span></h2>
<p>The optimal variational density <span class="math inline">\(q^\star(\sigma^2)\)</span> is given by
<span class="math display">\[
\begin{align*}
q^\star(\sigma^2) \propto \exp \left( E_{q(\mu), q(\beta)} \left( \log \left( p(\sigma^2 | \mathbf{y}, \mu, \beta ) \right) \right) \right) 
\end{align*}
\]</span>
which requires finding the full conditional distribution <span class="math inline">\(p(\sigma^2 | \mathbf{y}, \mu, \beta )\)</span>. The full conditional distribution is
<span class="math display">\[
\begin{align*}
p(\sigma^2 | \mathbf{y}, \mu, \beta ) &amp; \propto \operatorname{inverse-gamma} (a_{\sigma^2}, b_{\sigma^2}) \mbox{ where } \\
a_{\sigma^2} &amp; = \alpha_{\sigma^2} + \frac{n}{2} \\
b_{\sigma^2} &amp; = \beta_{\sigma^2} + \frac{1}{2} \sum_{i=1}^n \left( y_i - \mu - x_i \beta \right)^2
\end{align*}
\]</span>
Thus, the variational density is
<span class="math display">\[
\begin{align*}
q^\star(\sigma^2) &amp; \propto \exp \left( E_{q(\mu)} E_{q(\beta)} \left( \log \left( p(\sigma^2 | \mathbf{y}, \mu, \beta ) \right) \right) \right) \\
&amp; = \exp \left(  E_{q(\mu)}  E_{q(\beta)} \left( -\log(\sigma^2)^{-(\alpha_{\sigma^2} + \frac{n}{2}) - 1} - \frac{1}{\sigma^2} \left(\beta_{\sigma^2} + \frac{1}{2} \sum_{i=1}^n \left( y_i - \mu - x_i \beta \right)^2 \right) \right)  \right) \\
&amp; = \exp \left(  -\log(\sigma^2)^{-(\alpha_{\sigma^2} + \frac{n}{2}) - 1} - \frac{1}{\sigma^2} E_{q(\mu)}  E_{q(\beta)} \left( \beta_\sigma^2 + \frac{1}{2} \sum_{i=1}^n \left( y_i - \mu - x_i \beta \right)^2 \right) \right)  \\
&amp; = (\sigma^2)^{-(\alpha_{\sigma^2} + \frac{n}{2}) - 1} e^{- \frac{1}{\sigma^2} E_{q(\mu)}  E_{q(\beta)} \left( \beta_\sigma^2 + \frac{1}{2} \sum_{i=1}^n \left( y_i - \mu - x_i \beta \right)^2 \right) }  \\
\end{align*}
\]</span>
which is proportional to an inverse-gamma distribution with shape parameter
<span class="math display">\[
\begin{align*}
\widetilde{\alpha}_{\sigma^2} &amp; = \alpha_{\sigma^2} + \frac{n}{2} \\
\widetilde{\beta}_{\sigma^2} &amp; = E_{q(\mu)}  E_{q(\beta)} \left( \beta_\sigma^2 + \frac{1}{2} \sum_{i=1}^n \left( y_i - \mu - x_i \beta \right)^2 \right) 
\end{align*}
\]</span></p>
</div>
<div id="evaluating-the-expectations-inside-the-variational-densities" class="section level2">
<h2>Evaluating the expectations inside the variational densities</h2>
<p>In the above variational densities, there are expectations inside the distributions that must be evaluated. Letting <span class="math inline">\(\widetilde{\alpha}_{\sigma^2} = \alpha_{\sigma^2} + \frac{n}{2}\)</span> and
<span class="math inline">\(\widetilde{\beta}_{\sigma^2} = \beta_{\sigma^2} + E_{q(\mu)} E_{q(\beta)} \frac{1}{2} \sum_{i=1}^n \left( y_i - \mu - x_i \beta \right)^2\)</span></p>
<p><span class="math display">\[
\begin{align*}
E_{q(\sigma^2)} (\frac{1}{\sigma^2}) &amp; = \int_0^{\infty} \frac{1}{\sigma^2} q(\sigma^2) \\
&amp; = \int_0^{\infty} \frac{1}{\sigma^2}  \operatorname{inverse-gamma} ( \widetilde{\alpha}_{\sigma^2}, \widetilde{\beta}_{\sigma^2}) \\ 
&amp; = \int_0^{\infty} \frac{1}{\sigma^2} \left( \frac{\widetilde{\beta}_{\sigma^2}^{\widetilde{\alpha}_{\sigma^2}}}{\Gamma(\widetilde{\alpha}_{\sigma^2})} (\sigma^2)^{-\widetilde{\alpha}_{\sigma^2} - 1} e^{- \frac{\widetilde{\beta}_{\sigma^2}}{\sigma^2}} \right) \\
&amp; = \frac{\widetilde{\beta}_{\sigma^2}^{\widetilde{\alpha}_{\sigma^2}}}{\Gamma(\widetilde{\alpha}_{\sigma^2})} \int_0^{\infty} (\sigma^2)^{-\widetilde{\alpha}_{\sigma^2} - 2} e^{- \frac{\widetilde{\beta}_{\sigma^2}}{\sigma^2}} \\
&amp; = \frac{\widetilde{\beta}_{\sigma^2}^{\widetilde{\alpha}_{\sigma^2}}}{\Gamma(\widetilde{\alpha}_{\sigma^2})} \frac{\Gamma(\widetilde{\alpha}_{\sigma^2} + 1)}{\widetilde{\beta}_{\sigma^2}^{\widetilde{\alpha}_{\sigma^2} + 1}} \int_0^{\infty} \frac{\widetilde{\beta}_{\sigma^2}^{\widetilde{\alpha}_{\sigma^2} + 1}}{\Gamma(\widetilde{\alpha}_{\sigma^2} + 1)} (\sigma^2)^{-(\widetilde{\alpha}_{\sigma^2} + 1) - 1} e^{- \frac{\widetilde{\beta}_{\sigma^2}}{\sigma^2}} \\
&amp; = \frac{\widetilde{\beta}_{\sigma^2}^{\widetilde{\alpha}_{\sigma^2}}}{\Gamma(\widetilde{\alpha}_{\sigma^2})} \frac{\Gamma(\widetilde{\alpha}_{\sigma^2} + 1)}{\widetilde{\beta}_{\sigma^2}^{\widetilde{\alpha}_{\sigma^2} + 1}} 
\end{align*}
\]</span></p>
<p><span class="math inline">\(E_{q(\mu)} \left(\mu\right)\)</span></p>
</div>
</div>
<div id="example-multiple-linear-regression" class="section level1">
<h1>Example: Multiple Linear regression</h1>
<p>For <span class="math inline">\(i = 1, \ldots, n\)</span>, let <span class="math inline">\(y_i\)</span> be an observation with associated covariate vector <span class="math inline">\(\mathbf{x}_i\)</span> (including an intercept term). Then, the linear regression model can be written as
<span class="math display">\[
\begin{align*}
y_i &amp; \sim N(\mathbf{x}_i&#39; \boldsymbol{\beta}, \sigma^2)
\end{align*}
\]</span>
where the parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span> are assigned priors
<span class="math display">\[
\begin{align*}
\boldsymbol{\beta} &amp; \sim N(\boldsymbol{\mu}_\beta, \sigma^2 \boldsymbol{\Sigma}_\beta) \\
\sigma^2 &amp; \sim \operatorname{inverse-gamma}(\alpha_{\sigma^2}, \beta_{\sigma^2})
\end{align*}
\]</span></p>
<div id="variational-density-for" class="section level2">
<h2>Variational density for $</h2>
<p>The optimal variational density <span class="math inline">\(q^\star(\boldsymbol{\beta})\)</span> is given by
<span class="math display">\[
\begin{align*}
q^\star(\boldsymbol{\beta}) \propto \exp \left( E_{q(\sigma^2)} \left( \log \left( p(\boldsymbol{\beta} | \mathbf{y}, \sigma^2 ) \right) \right) \right)
\end{align*}
\]</span>
which requires finding the full conditional distribution <span class="math inline">\(p(\boldsymbol{\beta} | \mathbf{y}, \sigma^2 )\)</span>. The full conditional distribution is
<span class="math display">\[
\begin{align*}
p(\boldsymbol{\beta} | \mathbf{y}, \sigma^2 ) \propto N(\mathbf{A}^{-1} \mathbf{b}, \mathbf{A}^{-1}) \mbox{ where } \\
\mathbf{A} &amp; = \frac{1}{\sigma^2} \left( \mathbf{X}&#39;\mathbf{X} + \boldsymbol{\Sigma}_\beta^{-1} \right) \\
\mathbf{b} &amp; = \frac{1}{\sigma^2} \left( \mathbf{X}&#39;\mathbf{y} + \boldsymbol{\Sigma}_\beta^{-1} \boldsymbol{\mu}_\beta \right)
\end{align*}
\]</span>
Thus, the variational density is
<span class="math display">\[
\begin{align*}
q^\star(\boldsymbol{\beta}) &amp; \propto \exp \left( E_{q(\sigma^2)} \left( \log \left( p(\boldsymbol{\beta} | \mathbf{y}, \sigma^2 ) \right) \right) \right) \\
&amp; = \exp \left(  E_{q(\sigma^2)} \left( -\frac{1}{2} \left( \boldsymbol{\beta} - \mathbf{A}^{-1} \mathbf{b} \right)&#39; \mathbf{A}^{-1} \left( \boldsymbol{\beta} - \mathbf{A}^{-1} \mathbf{b} \right) \right) \right) \\
&amp; = \exp \left(  -\frac{1}{2} \left( \boldsymbol{\beta} - \mathbf{A}^{-1} \mathbf{b} \right)&#39; E_{q(\sigma^2)} (\sigma^2) \left( \mathbf{X}&#39;\mathbf{X} + \boldsymbol{\Sigma}_\beta^{-1} \right)^{-1}  \left( \boldsymbol{\beta} - \mathbf{A}^{-1} \mathbf{b} \right) \right) \\
\end{align*}
\]</span>
which is proportional to a Gaussian distribution</p>
</div>
</div>
